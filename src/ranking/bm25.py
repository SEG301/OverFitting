"""
BM25 (Best Matching 25) Ranking Algorithm - Code tay
=====================================================
Milestone 2 - SEG301: Search Engines & Information Retrieval

BM25 lÃ  thuáº­t toÃ¡n ranking dá»±a trÃªn xÃ¡c suáº¥t, cáº£i tiáº¿n tá»« TF-IDF.

CÃ´ng thá»©c BM25:
    score(D, Q) = Î£ IDF(qi) * (tf(qi, D) * (k1 + 1)) / (tf(qi, D) + k1 * (1 - b + b * |D| / avgdl))

Trong Ä‘Ã³:
    - tf(qi, D): Term frequency cá»§a query term qi trong document D
    - IDF(qi): Inverse Document Frequency = log((N - df + 0.5) / (df + 0.5) + 1)
    - N: Tá»•ng sá»‘ documents
    - df: Sá»‘ documents chá»©a term qi
    - |D|: Äá»™ dÃ i document D (sá»‘ terms)
    - avgdl: Äá»™ dÃ i trung bÃ¬nh cá»§a táº¥t cáº£ documents
    - k1: Tham sá»‘ Ä‘iá»u chá»‰nh term frequency saturation (máº·c Ä‘á»‹nh: 1.2)
    - b: Tham sá»‘ Ä‘iá»u chá»‰nh document length normalization (máº·c Ä‘á»‹nh: 0.75)

LÆ°u Ã½: KHÃ”NG sá»­ dá»¥ng báº¥t ká»³ thÆ° viá»‡n ranking nÃ o cÃ³ sáºµn.
Táº¥t cáº£ logic tÃ­nh toÃ¡n TF, IDF, BM25 score Ä‘á»u Ä‘Æ°á»£c code tay.

Kiáº¿n trÃºc tá»‘i Æ°u bá»™ nhá»›:
    - term_dict.pkl:  Dict[str, (df, offset, length)] â†’ ~18MB, load nhanh
    - postings.bin:   Binary file â†’ random access khi search
    - doc_lengths.pkl: Dict[int, int] â†’ ~12MB
    - doc_offsets.pkl: Dict[int, int] â†’ ~25MB, byte offset trong JSONL
    - Metadata Ä‘á»c on-demand tá»« file JSONL gá»‘c (khÃ´ng load vÃ o RAM)
"""

import os
import sys
import json
import math
import time
import pickle
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

try:
    from pyvi import ViTokenizer
except ImportError:
    ViTokenizer = None


# ============================================================================
# CONFIGURATION
# ============================================================================

# BM25 Parameters
K1 = 1.2    # Term frequency saturation parameter
B = 0.75    # Document length normalization parameter

# Paths
DEFAULT_DATA_PATH = os.path.join(os.path.dirname(__file__), "..", "..", "data")
DEFAULT_INDEX_DIR = os.path.join(DEFAULT_DATA_PATH, "index")
DEFAULT_JSONL_PATH = os.path.join(DEFAULT_DATA_PATH, "milestone1_fixed.jsonl")


# ============================================================================
# BM25 SEARCH ENGINE
# ============================================================================

class BM25Searcher:
    """
    BM25 Search Engine - Code tay hoÃ n toÃ n.
    
    Sá»­ dá»¥ng inverted index tá»« SPIMI (kiáº¿n trÃºc 2-file) Ä‘á»ƒ thá»±c hiá»‡n 
    ranked retrieval theo thuáº­t toÃ¡n BM25.
    
    Kiáº¿n trÃºc tá»‘i Æ°u bá»™ nhá»›:
    - Chá»‰ load term_dict (~18MB) + doc_lengths (~12MB) + doc_offsets (~25MB)
    - Tá»•ng RAM khi khá»Ÿi Ä‘á»™ng: ~55MB (thay vÃ¬ >3GB)
    - Postings: Ä‘á»c tá»« disk khi search (random access)
    - Metadata: Ä‘á»c tá»« JSONL khi cáº§n hiá»ƒn thá»‹ (random access)
    """
    
    def __init__(self, index_dir: str = DEFAULT_INDEX_DIR, 
                 jsonl_path: str = DEFAULT_JSONL_PATH,
                 k1: float = K1, b: float = B):
        self.index_dir = index_dir
        self.jsonl_path = jsonl_path
        self.k1 = k1
        self.b = b
        
        self.term_dict = None         # term -> (df, offset, length)
        self.postings_file = None     # file handle for postings.bin
        self.doc_lengths = None       # doc_id -> document length
        self.doc_offsets = None       # doc_id -> byte offset in JSONL
        self.jsonl_file = None        # file handle for JSONL
        self.total_docs = 0           # N
        self.avg_doc_length = 0.0     # avgdl
        self.vocab_size = 0
        
        self._loaded = False
    
    def load_index(self):
        """
        Load index files nháº¹ vÃ o RAM.
        Tá»•ng ~55MB: term_dict + doc_lengths + doc_offsets.
        """
        print("Loading index files...")
        load_start = time.time()
        
        # 1. Load term dictionary (~18MB)
        dict_path = os.path.join(self.index_dir, "term_dict.pkl")
        if not os.path.exists(dict_path):
            raise FileNotFoundError(
                f"Term dictionary not found: {dict_path}\n"
                f"Run spimi.py and merging.py first!"
            )
        with open(dict_path, "rb") as f:
            self.term_dict = pickle.load(f)
        self.vocab_size = len(self.term_dict)
        print(f"  [OK] Term dictionary: {self.vocab_size:,d} terms")
        
        # 2. Má»Ÿ postings file handle (khÃ´ng load)
        postings_path = os.path.join(self.index_dir, "postings.bin")
        self.postings_file = open(postings_path, "rb")
        print(f"  [OK] Postings file opened (random access)")
        
        # 3. Load document lengths (~12MB) 
        lengths_path = os.path.join(self.index_dir, "doc_lengths.pkl")
        with open(lengths_path, "rb") as f:
            self.doc_lengths = pickle.load(f)
        self.total_docs = len(self.doc_lengths)
        total_length = sum(self.doc_lengths.values())
        self.avg_doc_length = total_length / self.total_docs if self.total_docs > 0 else 0
        print(f"  [OK] Document lengths: {self.total_docs:,d} docs (avg: {self.avg_doc_length:.1f})")
        
        # 4. Load document offsets (~25MB)
        offsets_path = os.path.join(self.index_dir, "doc_offsets.pkl")
        if os.path.exists(offsets_path):
            with open(offsets_path, "rb") as f:
                self.doc_offsets = pickle.load(f)
            print(f"  [OK] Document offsets loaded (for metadata lookup)")
        else:
            self.doc_offsets = None
            print(f"  âš  doc_offsets.pkl not found (no metadata display)")
        
        # 5. Má»Ÿ JSONL file handle á»Ÿ binary mode (Ä‘á»ƒ byte offset khá»›p vá»›i SPIMI)
        if os.path.exists(self.jsonl_path):
            self.jsonl_file = open(self.jsonl_path, "rb")
            print(f"  [OK] JSONL file opened (metadata on-demand)")
        
        load_time = time.time() - load_start
        print(f"  Load time: {load_time:.1f}s")
        
        self._loaded = True
    
    def _get_postings(self, term: str) -> Optional[List[Tuple[int, int]]]:
        """
        Äá»c postings list cho 1 term tá»« binary file.
        Random file seek O(1).
        """
        if term not in self.term_dict:
            return None
        
        df, offset, length = self.term_dict[term]
        self.postings_file.seek(offset)
        postings_bytes = self.postings_file.read(length)
        return pickle.loads(postings_bytes)
    
    def _get_doc_metadata(self, doc_id: int) -> dict:
        """
        Äá»c metadata cá»§a 1 document tá»« file JSONL gá»‘c.
        Sá»­ dá»¥ng byte offset Ä‘á»ƒ seek trá»±c tiáº¿p (khÃ´ng scan toÃ n bá»™ file).
        """
        if not self.doc_offsets or not self.jsonl_file:
            return {}
        
        if doc_id not in self.doc_offsets:
            return {}
        
        try:
            byte_offset = self.doc_offsets[doc_id]
            self.jsonl_file.seek(byte_offset)
            raw_line = self.jsonl_file.readline()
            line = raw_line.decode("utf-8", errors="ignore")
            doc = json.loads(line)
            return {
                "company_name": doc.get("company_name", ""),
                "tax_code": doc.get("tax_code", ""),
                "address": doc.get("address", ""),
                "representative": doc.get("representative", ""),
                "status": doc.get("status", ""),
                "industries_str_seg": doc.get("industries_str_seg", ""),
            }
        except Exception:
            return {}
    
    def compute_idf(self, term: str) -> float:
        """
        TÃ­nh IDF (Inverse Document Frequency) cho má»™t term.
        
        CÃ´ng thá»©c IDF theo BM25 (Robertson-Sparck Jones):
            IDF(t) = log((N - df(t) + 0.5) / (df(t) + 0.5) + 1)
        """
        if term not in self.term_dict:
            return 0.0
        
        df = self.term_dict[term][0]
        N = self.total_docs
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        return idf
    
    def compute_tf_component(self, tf: int, doc_length: int) -> float:
        """
        TÃ­nh thÃ nh pháº§n TF trong cÃ´ng thá»©c BM25.
        
        CÃ´ng thá»©c:
            tf_component = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |D| / avgdl))
        """
        length_norm = 1 - self.b + self.b * (doc_length / self.avg_doc_length)
        tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * length_norm)
        return tf_component
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, dict]]:
        """
        TÃ¬m kiáº¿m vÃ  xáº¿p háº¡ng documents theo BM25.
        
        Quy trÃ¬nh:
        1. Tokenize query
        2. Vá»›i má»—i query term: tÃ­nh IDF, Ä‘á»c postings (random access tá»« disk)
        3. Vá»›i má»—i document: tÃ­nh BM25 score = Î£ IDF * TF_component
        4. Tráº£ vá» top-k documents kÃ¨m metadata
        
        Tá»‘i Æ°u: Skip terms quÃ¡ phá»• biáº¿n (df > 30% corpus) vÃ¬:
        - IDF ráº¥t tháº¥p â†’ Ä‘Ã³ng gÃ³p score khÃ´ng Ä‘Ã¡ng ká»ƒ
        - Postings list cá»±c lá»›n (hÃ ng triá»‡u entries) â†’ unpickle ráº¥t cháº­m
        """
        if not self._loaded:
            self.load_index()
        
        search_start = time.time()
        
        query_tokens = self._tokenize_query(query)
        if not query_tokens:
            return []
        
        # NgÆ°á»¡ng df tá»‘i Ä‘a: term xuáº¥t hiá»‡n >30% docs sáº½ bá»‹ bá» qua
        max_df = int(self.total_docs * 0.3)
        
        # Accumulate BM25 scores
        doc_scores = defaultdict(float)
        skipped_terms = []
        
        for term in query_tokens:
            if term not in self.term_dict:
                continue
            
            df = self.term_dict[term][0]
            
            # Skip terms quÃ¡ phá»• biáº¿n (postings list quÃ¡ lá»›n)
            if df > max_df:
                skipped_terms.append(f"{term}(df={df:,d})")
                continue
            
            idf = self.compute_idf(term)
            if idf <= 0:
                continue
            
            postings = self._get_postings(term)
            if postings is None:
                continue
            
            for doc_id, tf in postings:
                doc_length = self.doc_lengths.get(doc_id, self.avg_doc_length)
                tf_comp = self.compute_tf_component(tf, doc_length)
                doc_scores[doc_id] += idf * tf_comp
        
        # Top-k
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        top_docs = sorted_docs[:top_k]
        
        # Gáº¯n metadata (Ä‘á»c on-demand tá»« JSONL)
        results = []
        for doc_id, score in top_docs:
            metadata = self._get_doc_metadata(doc_id)
            results.append((doc_id, score, metadata))
        
        search_time = time.time() - search_start
        
        print(f"\n  Query: \"{query}\"")
        print(f"  Tokens: {query_tokens}")
        if skipped_terms:
            print(f"  Skipped (too common): {', '.join(skipped_terms)}")
        print(f"  Documents matched: {len(doc_scores):,d}")
        print(f"  Search time: {search_time*1000:.1f}ms")
        
        return results
    
    def _tokenize_query(self, query: str) -> List[str]:
        """
        Tokenize query text.
        Tá»± Ä‘á»™ng sá»­ dá»¥ng PyVi Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t náº¿u cÃ³ thá»ƒ.
        """
        processed_query = query.lower()
        
        # Náº¿u cÃ³ PyVi, thá»±c hiá»‡n tÃ¡ch tá»« tá»± Ä‘á»™ng (vd: "cÃ´ng nghá»‡" -> "cÃ´ng_nghá»‡")
        if ViTokenizer:
            processed_query = ViTokenizer.tokenize(processed_query)
            
        tokens = processed_query.split()
        result = []
        for token in tokens:
            if len(token) <= 1 and not token.isdigit():
                continue
            if all(c in '.,;:!?()-_/\\|@#$%^&*+=<>{}[]"\'~`' for c in token):
                continue
            result.append(token)
        return result
    
    def get_stats(self) -> dict:
        """Tráº£ vá» thá»‘ng kÃª vá» index."""
        if not self._loaded:
            self.load_index()
        return {
            "total_documents": self.total_docs,
            "vocabulary_size": self.vocab_size,
            "avg_document_length": self.avg_doc_length,
            "k1": self.k1,
            "b": self.b,
        }
    
    def close(self):
        """ÄÃ³ng file handles."""
        if self.postings_file:
            self.postings_file.close()
            self.postings_file = None
        if self.jsonl_file:
            self.jsonl_file.close()
            self.jsonl_file = None
    
    def __del__(self):
        self.close()


# ============================================================================
# DISPLAY UTILS
# ============================================================================

def display_results(results: List[Tuple[int, float, dict]], query: str):
    """Hiá»ƒn thá»‹ káº¿t quáº£ tÃ¬m kiáº¿m."""
    print("\n" + "=" * 80)
    print(f"  ðŸ” Search Results for: \"{query}\"")
    print(f"  Found {len(results)} results")
    print("=" * 80)
    
    if not results:
        print("  No results found.")
        return
    
    for rank, (doc_id, score, meta) in enumerate(results, 1):
        print(f"\n  #{rank} [Score: {score:.4f}] (Doc ID: {doc_id})")
        print(f"  â”œâ”€ Company:  {meta.get('company_name', 'N/A')}")
        print(f"  â”œâ”€ Tax Code: {meta.get('tax_code', 'N/A')}")
        print(f"  â”œâ”€ Address:  {meta.get('address', 'N/A')}")
        print(f"  â”œâ”€ Rep:      {meta.get('representative', 'N/A')}")
        print(f"  â”œâ”€ Status:   {meta.get('status', 'N/A')}")
        industries = meta.get('industries_str_seg', 'N/A')
        if industries and len(industries) > 150:
            industries = industries[:150] + "..."
        print(f"  â””â”€ Industry: {industries}")
    
    print("\n" + "=" * 80)


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    data_dir = os.path.join(os.path.dirname(__file__), "..", "..", "data")
    index_dir = os.path.join(data_dir, "index")
    jsonl_path = os.path.join(data_dir, "milestone1_fixed.jsonl")
    
    searcher = BM25Searcher(index_dir=index_dir, jsonl_path=jsonl_path)
    searcher.load_index()
    
    stats = searcher.get_stats()
    print(f"\nðŸ“Š Index Statistics:")
    for key, val in stats.items():
        print(f"  {key}: {val:,}" if isinstance(val, int) else f"  {key}: {val}")
    
    test_queries = [
        "cÃ´ng_ty cÃ´ng_nghá»‡ thÃ´ng_tin",
        "báº¥t_Ä‘á»™ng_sáº£n hÃ _ná»™i",
        "xuáº¥t_kháº©u thá»§y_sáº£n",
    ]
    
    for q in test_queries:
        results = searcher.search(q, top_k=5)
        display_results(results, q)
    
    searcher.close()
