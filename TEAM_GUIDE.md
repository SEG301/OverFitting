# HÆ°á»›ng dáº«n phÃ¢n chia cÃ´ng viá»‡c - Team 3 ngÆ°á»i
## SEG301 - Milestone 1: Data Acquisition

---

## ğŸ¯ Má»¥c tiÃªu: 1.000.000 documents

| ThÃ nh viÃªn | Nguá»“n | Æ¯á»›c tÃ­nh | Tráº¡ng thÃ¡i |
|------------|-------|----------|------------|
| **Member 1** | Masothue (NgÃ nh 1-25) | ~350,000 | â³ |
| **Member 2** | Masothue (NgÃ nh 26-73) | ~450,000 | â³ |
| **Member 3** | Hosocongty + Reviewcongty | ~200,000 | â³ |

---

## ğŸ“‹ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t (Má»—i thÃ nh viÃªn Ä‘á»u lÃ m)

### 1. Clone repository
```bash
git clone https://github.com/SEG301/OverFitting.git
cd OverFitting
```

### 2. Táº¡o Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 4. Test cÃ i Ä‘áº·t
```bash
python -c "from src.crawler import MasothueCrawler; print('OK')"
```

---

## ğŸ‘¤ Member 1: Masothue - NgÃ nh 1-25

### Cháº¡y crawler
```bash
# Cháº¡y full (sáº½ máº¥t 1-2 ngÃ y)
python -m src.crawler.main crawl --source masothue --industries 1-25 --output data_member1

# Hoáº·c test trÆ°á»›c vá»›i limit
python -m src.crawler.main crawl --source masothue --industries 1-25 --limit 1000 --output data_member1
```

### Resume náº¿u bá»‹ giÃ¡n Ä‘oáº¡n
```bash
python -m src.crawler.main crawl --source masothue --industries 1-25 --output data_member1 --resume
```

### Kiá»ƒm tra tiáº¿n Ä‘á»™
- File checkpoint: `data_member1/masothue_checkpoint.json`
- Xem log: `crawler.log`

---

## ğŸ‘¤ Member 2: Masothue - NgÃ nh 26-73

### Cháº¡y crawler
```bash
# Cháº¡y full
python -m src.crawler.main crawl --source masothue --industries 26-73 --output data_member2

# Test trÆ°á»›c
python -m src.crawler.main crawl --source masothue --industries 26-73 --limit 1000 --output data_member2
```

---

## ğŸ‘¤ Member 3: Hosocongty + Reviewcongty

### Cháº¡y crawler
```bash
# Cháº¡y Hosocongty
python3 -m src.crawler.main crawl --source hosocongty --output data_member3

# Cháº¡y Reviewcongty
python3 -m src.crawler.main crawl --source reviewcongty --output data_member3
```

---

## ğŸ”„ Sau khi crawl xong - Merge dá»¯ liá»‡u

### 1. Thu tháº­p files tá»« cÃ¡c thÃ nh viÃªn
Má»—i ngÆ°á»i upload folder `data_memberX/` lÃªn Google Drive hoáº·c copy sang mÃ¡y leader.

### 2. Merge táº¥t cáº£ dá»¯ liá»‡u
```bash
# Gá»™p táº¥t cáº£ file JSONL
python -m src.crawler.main merge \
    --masothue "data_member1/*.jsonl" "data_member2/*.jsonl" \
    --hosocongty "data_member3/hosocongty*.jsonl" \
    --reviewcongty "data_member3/reviewcongty*.jsonl" \
    --output data/all_companies_merged.jsonl
```

### 3. Loáº¡i bá» trÃ¹ng láº·p
```bash
python -m src.crawler.main dedup \
    --input "data/all_companies_merged.jsonl" \
    --output "data/all_companies_final.jsonl" \
    --key tax_code
```

### 4. Táº¡o thá»‘ng kÃª
```bash
python -m src.crawler.main stats \
    --input "data/all_companies_final.jsonl" \
    --output "docs/data_statistics.md"
```

---

## ğŸ“Š Checklist trÆ°á»›c khi ná»™p

### Vá» dá»¯ liá»‡u
- [ ] Äá»§ 1.000.000 documents
- [ ] KhÃ´ng cÃ³ duplicate (Ä‘Ã£ cháº¡y dedup)
- [ ] Text Ä‘Ã£ Ä‘Æ°á»£c segment (cÃ³ field `_segmented`)
- [ ] File lÆ°u dáº¡ng JSONL

### Vá» code
- [ ] CÃ³ xá»­ lÃ½ async/multi-thread âœ…
- [ ] CÃ³ cÆ¡ cháº¿ resume (checkpoint) âœ…
- [ ] Code module hÃ³a (nhiá»u file) âœ…

### Vá» GitHub
- [ ] Commit Ä‘á»u Ä‘áº·n (má»—i ngÃ y Ã­t nháº¥t 1 commit)
- [ ] `ai_log.md` Ä‘Æ°á»£c cáº­p nháº­t
- [ ] `README.md` cÃ³ link táº£i dataset

### Vá» bÃ¡o cÃ¡o
- [ ] `docs/Milestone1_Report.pdf`
- [ ] Thá»‘ng kÃª sá»‘ lÆ°á»£ng tá»« vá»±ng
- [ ] Äá»™ dÃ i trung bÃ¬nh documents

---

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **KHÃ”NG upload file dá»¯ liá»‡u lÃªn GitHub** - File 1 triá»‡u dÃ²ng quÃ¡ lá»›n!
2. **Upload dá»¯ liá»‡u lÃªn Google Drive** vÃ  Ä‘á»ƒ link trong README
3. **Commit code thÆ°á»ng xuyÃªn** - YÃªu cáº§u báº¯t buá»™c cá»§a mÃ´n!
4. **Cáº­p nháº­t ai_log.md** má»—i khi chat vá»›i AI

---

## ğŸ†˜ Troubleshooting

### Bá»‹ block IP
```bash
# Giáº£m concurrent requests
python3 -m src.crawler.main crawl --source masothue --concurrent 10

# Hoáº·c tÄƒng rate limit (cháº­m hÆ¡n)
python3 -m src.crawler.main crawl --source masothue --delay 2.0
```

### Lá»—i RAM khÃ´ng Ä‘á»§
- ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c
- Giáº£m `max_concurrent` xuá»‘ng 20

### Resume khÃ´ng hoáº¡t Ä‘á»™ng
- Kiá»ƒm tra file checkpoint cÃ³ tá»“n táº¡i khÃ´ng
- XÃ³a checkpoint Ä‘á»ƒ crawl láº¡i tá»« Ä‘áº§u:
```bash
rm data_memberX/*_checkpoint.json
```
