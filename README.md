# SEG301 - PROJECT MILESTONE 1
**Topic:** Business Information Retrieval
**Group:** OverFitting

## Project Overview
This project implements a high-performance web crawler to acquire business registration data from public sources. The goal is to collect, clean, and standardize over 1,000,000 business records for building a Vertical Search Engine.

## Data Acquisition Statistics
- **Source:** InfoDoanhNghiep
- **Total Raw Records:** 8,177,481
- **Total Unique Records:** 1,298,633 (De-duplicated by Tax Code & Name)
- **Vocabulary Size:** 393,194 words
- **Average Document Length:** 19.24 words

## ğŸ’¾ Dataset Download
Do dung lÆ°á»£ng dá»¯ liá»‡u lá»›n (>800MB), chÃºng tÃ´i lÆ°u trá»¯ Full Dataset trÃªn Google Drive.

ğŸ‘‰ **Download Link:** [Full Dataset Milestone 1 (Google Drive)](https://drive.google.com/drive/folders/1XdAX7aw-ibpCniuHVyMNmUkD9JHv-dK-?usp=sharing)

*File máº«u xem trÆ°á»›c trÃªn Github:* `data_sample/sample.jsonl` (50 records).

## Repository Structure
```
SEG301-OverFitting/
â”œâ”€â”€ .gitignore               # Git configuration
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ ai_log.md                # AI Interaction Log (Audit trail)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ docs/                    # Documentation & Reports
â”‚   â””â”€â”€ Milestone1_Report.md
â”œâ”€â”€ data_sample/             # Data samples for grading
â”‚   â””â”€â”€ sample.jsonl         # Sample dataset
â””â”€â”€ src/                     # Source Code
    â””â”€â”€ crawler/
        â”œâ”€â”€ speed_crawler.py # Multi-threaded Crawler
        â””â”€â”€ final_process.py # Data Cleaning Pipeline
```

## Setup & Usage

### Prerequisites
- Python 3.10+
- Recommended: Visual Studio Code

### Installation
1. Clone the repository.
2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: .\venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running the Crawler
To start the acquisition process (Nationwide sweep):
```bash
python src/crawler/speed_crawler.py
```
*Note: The crawler uses 30 concurrent threads. Ensure stable network connection.*

### Data Processing
To merge, de-duplicate, and segment words:
```bash
python src/crawler/final_process.py
```
Output will be saved to `data/milestone1_final.jsonl` (Local only, not git-tracked).

## Data Format (JSONL)
Each line in the dataset represents a unique business record:
```json
{
  "company_name": "CÃ”NG TY TNHH VÃ Dá»¤",
  "tax_code": "0101234567",
  "address": "Sá»‘ 1, ÄÆ°á»ng A, Quáº­n B, TP. HÃ  Ná»™i",
  "source": "InfoDoanhNghiep",
  "url": "https://infodoanhnghiep.com/...",
  "company_name_seg": "CÃ”NG_TY TNHH VÃ_Dá»¤",
  "address_seg": "Sá»‘ 1 , ÄÆ°á»ng A , Quáº­n B , TP . HÃ _Ná»™i"
}
```

---
**Course:** SEG301 - Search Engines & Information Retrieval
**Mileage:** Milestone 1 (Week 4)
